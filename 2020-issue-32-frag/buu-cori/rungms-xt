#!/bin/csh
#
#  last update = 17 Aug 2016
#
#  This is a C-shell script to execute GAMESS, by typing
#       rungms JOB VERNO NCPUS >& JOB.log &
#  JOB    is the name of the 'JOB.inp' file to be executed,
#  VERNO  is the number of the executable you chose at 'lked' time,
#  NCPUS  is the number of processors to be used, or the name of
#         a host list file.
#
#  Unfortunately execution is harder to standardize than compiling,
#  so you have to do a bit more than name your machine type here:
#
#    a) choose the target for execution from the following list:
#           sockets, mpi, ga, altix, cray-xt, ibm64-sp, sgi64
#       IBM Blue Gene uses separate execution files: ~/gamess/machines/ibm-bg
#
#       choose "sockets" if your compile time target was any of these:
#             axp64, hpux32, hpux64, ibm32, ibm64, linux32,
#             mac32, mac64, sgi32, sun32, sun64
#       as all of these systems use TCP/IP sockets.  Do not name your
#       specific compile time target, instead choose "sockets".
#
#       If your target was 'linux64', you may chose "sockets" or "mpi",
#       according to how you chose to compile.  The MPI example below
#       should be carefully matched against info found in 'readme.ddi'!
#
#       Choose 'ga' if and only if you did a 'linux64' build linked
#       to the LIBCCHEM software for CPU/GPU computations.
#
#           Search on the words typed in capital letters just below
#           in order to find the right place to choose each one:
#    b) choose a directory SCR where large temporary files can reside.
#       This should be the fastest possible disk access, very spacious,
#       and almost certainly a local disk.
#       Translation: do not put these files on a slow network file system!
#    c) choose a directory USERSCR on the file server where small ASCII
#       supplementary output files should be directed.
#       Translation: it is OK to put this on a network file system!
#    d) name the location GMSPATH of your GAMESS binary.
#    e) change the the VERNO default to the version number you chose when
#       running "lked" as the VERNO default, and maybe NCPUS' default.
#    f) make sure that the ERICFMT file name and MCPPATH pathname point to
#       your file server's GAMESS tree, so that all runs can find them.
#       Again, a network file system is quite OK for these two.
#    g) customize the execution section for your target below,
#       each has its own list of further requirements.
#    h) it is unwise to have every user take a copy of this script, as you
#       can *NEVER* update all the copies later on.  Instead, it is better
#       to ask other users to create an alias to point to a common script,
#       such as this in their C-shell .login file,
#             alias gms '/u1/mike/gamess/rungms'
#    i) it is entirely possible to make 'rungms' run in a batch queue,
#       be it PBS, DQS, et cetera.  This is so installation dependent
#       that we leave it to up to you, although we give examples.
#       See ~/gamess/tools, where there are two examples of "front-end"
#       scripts which can use this file as the "back-end" actual job.
#       We use the front-end "gms" on local Infiniband clusters using
#       both Sun Grid Engine (SGE), and Portable Batch System (PBS).
#       See also a very old LoadLeveler "ll-gms" for some IBM systems.
#
set TARGET=cray-xc
set SCR=$SCRATCH/gms
set USERSCR=$SLURM_SUBMIT_DIR/gms
#set GMSPATH=/usr/common/software/gamess/18AUG2016R1/bin
#set GMSPATH=/global/homes/b/buupq/FMOfromMV
set GMSPATH=/global/homes/b/buupq/EFMOrimp2
##set GMSPATH="$HOME/consulting/INC0100244/gamess"
# ankitb: SCR is scratch space used during the run;
# ankitb: USERSCR is for backing stuff up after the run
#
# Following lines from hjw's build on edison
# hjw the next 3 lines added Fri Nov 30 10:28:32 PST 2012
# to help with running multiple gamess's:
set TAG=`date +"%F_%H%M%S"`
set SCR=$SCR-$TAG
set USERSCR=$USERSCR-$TAG
echo "GAMESS temporary binary files will be written to $SCR"
echo "GAMESS supplementary output files will be written to $USERSCR"
mkdir -p $SCR
mkdir -p $USERSCR

set JOB=$1      # name of the input file xxx.inp, give only the xxx part
set VERNO=00    # revision number of the executable created by 'lked' step
set NCPUS=$2    # number of compute processes to be run
#
# provide defaults if last two arguments are not given to this script
if (null$VERNO == null) set VERNO=01
if (null$NCPUS == null) set NCPUS=1
#
#  ---- the top third of the script is input and other file assignments ----
#
echo "----- GAMESS execution script 'rungms' -----"
set master=`hostname`
echo This job is running on host $master
echo under operating system `uname` at `date`

echo "Slurm has assigned the following compute nodes to this run:"
echo "$SLURM_JOB_NODELIST"
#
echo "Available scratch disk space (Kbyte units) at beginning of the job is"
df -k $SCR
echo "GAMESS temporary binary files will be written to $SCR"
echo "GAMESS supplementary output files will be written to $USERSCR"

#  Grab a copy of the input file.
#  In the case of examNN jobs, file is in tests/standard subdirectory.
#  In the case of exam-vbNN jobs, file is in vb2000's tests subdirectory.
if ($JOB:r.inp == $JOB) set JOB=$JOB:r      # strip off possible .inp
echo "Copying input file $JOB.inp to your run's scratch directory..."
if (-e $JOB.inp) then
   set echo
   cp  $JOB.inp  $SCR/$JOB.F05
   unset echo
else
   if (-e tests/standard/$JOB.inp) then
      set echo
      cp  tests/standard/$JOB.inp  $SCR/$JOB.F05
      unset echo
   else
      if (-e tests/$JOB.inp) then
         set echo
         cp  tests/$JOB.inp  $SCR/$JOB.F05
         unset echo
      else
         echo "Input file $JOB.inp does not exist."
         echo "This job expected the input file to be in directory `pwd`"
         echo "Please fix your file name problem, and resubmit."
         exit 4
      endif
   endif
endif

#    define many environment variables setting up file names.
#    anything can be overridden by a user's own choice, read 2nd.
#
source $GMSPATH/gms-files.csh
if (-e $HOME/.gmsrc) then
   echo "reading your own $HOME/.gmsrc"
   source $HOME/.gmsrc
endif
#
#    In case GAMESS has been interfaced to the Natural Bond Orbital
#    analysis program (http://www.chem.wisc.edu/~nbo6), you must
#    specify the full path name to the NBO binary.
#    This value is ignored if NBO has not been linked to GAMESS.
#
# setenv NBOEXE /u1/mike/nbo6/bin/nbo6.i8.exe
#
#        choose remote shell execution program.
#    Parallel run do initial launch of GAMESS on remote nodes by the
#    following program.  Note that the authentication keys for ssh
#    must have been set up correctly.
#    If you wish, choose 'rsh/rcp' using .rhosts authentication instead.
setenv DDI_RSH ssh
setenv DDI_RCP scp
#
#    If a $GDDI input group is present, the calculation will be using
#    subgroups within DDI (the input NGROUP=0 means this isn't GDDI).
#
#    The master within each group must have a copy of INPUT, which is
#    dealt with below (prior to execution), once we know something about
#    the host names where INPUT is required.  The INPUT does not have
#    the global rank appended to its name, unlike all other files.
#
#    OUTPUT and PUNCH (and perhaps many other files) are opened on all
#    processes (not just the master in each subgroup), but unique names
#    will be generated by appending the global ranks.  Note that OUTPUT
#    is not opened by the master in the first group, but is used by all
#    other groups.  Typically, the OUTPUT from the first group's master
#    is the only one worth saving, unless perhaps if runs crash out.
#
#    The other files that GDDI runs might use are already defined above.
#
set ngddi=`grep -i '^ \$GDDI' $SCR/$JOB.F05 | grep -iv 'NGROUP=0 ' | wc -l`
if ($ngddi > 0) then
   set GDDIjob=true
   echo "This is a GDDI run, keeping various output files on local disks"
   set echo
   setenv  OUTPUT $SCR/$JOB.F06
   setenv   PUNCH $SCR/$JOB.F07
   unset echo
else
   set GDDIjob=false
endif

#             replica-exchange molecular dynamics (REMD)
#     option is active iff runtyp=md as well as mremd=1 or 2.
#     It utilizes multiple replicas, one per subgroup.
#     Although REMD is indeed a GDDI kind of run, it handles its own
#     input file manipulations, but should do the GDDI file defs above.
set runmd=`grep -i runtyp=md $SCR/$JOB.F05 | wc -l`
set mremd=`grep -i mremd= $SCR/$JOB.F05 | grep -iv 'mremd=0 ' | wc -l`
if (($mremd > 0) && ($runmd > 0) && ($ngddi > 0)) then
   set GDDIjob=false
   set REMDjob=true
   echo "This is a REMD run, keeping various output files on local disks"
   set echo
   setenv TRAJECT     $SCR/$JOB.F04
   setenv RESTART $USERSCR/$JOB.rst
   setenv    REMD $USERSCR/$JOB.remd
   unset echo
   set GDDIinp=(`grep -i '^ \$GDDI' $JOB.inp`)
   set numkwd=$#GDDIinp
   @ g = 2
   @ gmax = $numkwd - 1
   while ($g <= $gmax)
      set keypair=$GDDIinp[$g]
      set keyword=`echo $keypair | awk '{split($1,a,"="); print a[1]}'`
      if (($keyword == ngroup) || ($keyword == NGROUP)) then
         set nREMDreplica=`echo $keypair | awk '{split($1,a,"="); print a[2]}'`
         @ g = $gmax
      endif
      @ g++
   end
   unset g
   unset gmax
   unset keypair
   unset keyword
else
   set REMDjob=false
endif

#    data left over from a previous run might be precious, stop if found.
if ((-e $PUNCH) || (-e $MAKEFP) || (-e $TRAJECT) || (-e $RESTART) ) then
   echo "Please save, rename, or erase these files from a previous run:"
   echo "     $PUNCH,"
   echo "     $TRAJECT,"
   echo "     $RESTART, and/or"
   echo "     $MAKEFP,"
   echo "and then resubmit this computation."
   exit 4
endif

#  ---- the middle third of the script is to execute GAMESS ----
#
#  we show execution sections that should work for
#        sockets, mpi, altix, cray-xt, ibm64-sp, sgi64
#  and then two others
#        cray-x1, necsx
#  which are not mentioned at the top of this file, as they are quite stale.
#
#   Most workstations run DDI over TCP/IP sockets, and therefore execute
#   according to the following clause.  The installer must
#      a) Set the path to point to the DDIKICK and GAMESS executables.
#      b) Build the HOSTLIST variable as a word separated string, i.e. ()'s.
#         There should be one host name for every compute process that is
#         to be run.  DDIKICK will automatically generate a set of data
#         server processes (if required) on the same hosts.
#   An extended explanation of the arguments to ddikick.x can be found
#   in the file gamess/ddi/readme.ddi, if you have any trouble executing.


#   CRAY-XT (various models) running GAMESS/DDI over MPI wants you to
#      a) set the path to point to the GAMESS executable
#      b) set SMP_SIZE to the number of cores in each XT node
#      c) read the notes below about SCR and USERSCR
#
#   This machine runs only one MPI process/core, with most of these
#   able to be compute processes.  DDI_DS_PER_NODE lets you pick
#   how many of processes are to function as data servers.
#   So a node runs SMP_SIZE minus DDI_DE_PER_NODE compute processes.
#
#   The TPN variable below lets you use more memory, by wasting
#   some of the processors, if that is needed to do your run.
#   The 4th run parameter has to be passed at time of job submission,
#   if not, all cores are used.
#
#   This machine may not allow FORTRAN to access the file server
#   directly.  As a work-around, input data like the error function
#   table can to be copied to the working disk SCR.  Any extra
#   output files can be rescued from USERSCR after the run ends.
#
#   For speed reasons, you probably want to set SCR at the top of this
#   file to /tmp, which is a RAM disk.  Not all data centers will let
#   you do this, and it is acceptable to use the less eficient
#   alternative of setting SCR to a /lustre subdirectory.
#
#   You should set USERSCR to your directory in /lustre, which is
#   visible to all compute nodes, but not as fast as its /tmp.
#   Supplemental output files (like .dat) are then not in a RAM
#   disk which is wiped automatically at job end.
#
#   If you use subgroups, e.g. $GDDI input for FMO runs, you should
#   modify the input copying near the top of this file to copy to
#   USERSCR rather than SCR.  A file in /lustre is visible to all
#   nodes!  You must also change gms-files.csh to define INPUT as
#   being in USERSCR rather than SCR.
#
#   aprun flags:
#   -n is number of processing elements PEs required for the application
#   -N is number of MPI tasks per physical node
#   -d is number of threads per MPI task (interacts w/ OMP_NUM_THREADS)
#   -r is number of CPU cores to be used for core specialization
#   -j is number of CPUs to use per compute unit (single stream mode)
#   If your data center does not let you use -r 1 below, to run on
#   a jitter-free microkernel, just remove that flag from 'aprun'.
#
if ($TARGET == cray-xc) then
       #   path to binary, and number of cores per node.
   set SMP_SIZE=32

       # number of processes per node (TPN=tasks/node)
   set TPN=$3
   if (null$TPN == null) set TPN=$SMP_SIZE
   if ($TPN > $SMP_SIZE) set TPN=$SMP_SIZE

   if (!(-e $SCR/$JOB)) mkdir $SCR/$JOB

       # copy auxiliary data files to working disk, redefine their location.
   cp    $ERICFMT $SCR/$JOB/ericfmt.dat
   cp -r $MCPPATH $SCR/$JOB/MCP
   setenv ERICFMT $SCR/$JOB/ericfmt.dat
   setenv MCPPATH $SCR/$JOB/MCP

       # execute, with a few run-time tunings set first.
   set echo
   setenv DDI_DS_PER_NODE 1
   chdir $SCR/$JOB
   setenv DDI_LOGICAL_NODE_SIZE 1
   setenv OMP_NUM_THREADS 68
   setenv OMP_STACKSIZE 1000MB
   #srun -n $NCPUS -c 2 --cpu_bind=cores $GMSPATH/gamess.$VERNO.x $JOB
   srun -n $NCPUS -c 2 --cpu_bind=none $GMSPATH/gamess.$VERNO.x $JOB
   unset echo

#             Rescue the supplementary ASCII output files,
#             from /lustre to one's permanent disk storage.
#             This user is doing FMO trajectories, mainly,
#             and ends up saving all those files...
#   set PERMSCR=/u/sciteam/spruitt/scr
   if (-e $SCR/$JOB.efp)   cp -v $SCR/$JOB.efp   $USERSCR/$JOB.efp
   if (-e $SCR/$JOB.gamma) cp -v $SCR/$JOB.gamma $USERSCR/$JOB.gamma 
   if (-e $SCR/$JOB.trj)   cp -v $SCR/$JOB.trj   $USERSCR/$JOB.trj   
   if (-e $SCR/$JOB.rst)   cp -v $SCR/$JOB.rst   $USERSCR/$JOB.rst   
   if (-e $SCR/$JOB.dat)   cp -v $SCR/$JOB.dat   $USERSCR/$JOB.dat   
                           cp -v $SCR/$JOB.trj.000* $USERSCR/$JOB.trj.000*
   rm -f  $SCR/$JOB.*
#              clean SCR, e.g. the RAM disk /tmp
   rm -f  $SCR/$JOB/$JOB.F*
   rm -f  $SCR/$JOB/ericfmt.dat
   rm -rf $SCR/$JOB/MCP
   rmdir  $SCR/$JOB
#             perhaps these next things are batch queue related files?
#             this is dangerous if jobs are launched from home directory,
#             as it will wipe out input and output!
   #---rm -f  /u/sciteam/spruitt/$JOB.*
endif
#
#  and this is the end
#
date
time
exit
